 Things you can try out to fine-tune BERT


In this module, we fine-tuned a pre-trained BERT model with a twitter dataset. Now here are a few things that you can try to further improve the model's performance:

路 Fine-tune the bert-large model (only on colab) instead of the bert-base model

路 Try some other value for the max_length parameter in the BERT tokenizer

路 Try a batch size of 64

路 Try different values of the learning rate (0.0001, 0.005, 0.05, etc)

After trying out these suggestions, share your model's performance on the validation set.
